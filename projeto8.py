# Projeto 8 - IA com LLM e RAG Para Automatizar a Busca em Documentos Internos da Empresa

# Imports

# Biblioteca para extrair texto de arquivos .docx
import docx2txt  

# Biblioteca para criar aplica√ß√µes web interativas
import streamlit as st  

# Fun√ß√£o para importar o modelo de embeddings 
from langchain_community.embeddings import HuggingFaceEmbeddings

# Usaremos o Ollama para importar o LLM
from llama_index.llms.ollama import Ollama  

# Importa componentes essenciais do Llama Index para ler os documentos e gerar o banco de dados vetorial
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings 

# Filtra warnings
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√µes da p√°gina
st.set_page_config(page_title = "DSA Projeto 8", page_icon = ":mag_right:", layout = "centered")

# T√≠tulo e subt√≠tulo da aplica√ß√£o
st.title("üîç Busca Inteligente em Documentos Internos")
st.subheader("LLMs e RAG Para Automatizar a Busca em Documentos Internos da Empresa")

# Texto de introdu√ß√£o
st.markdown("""
Bem-vindo ao sistema de busca automatizada para documentos internos da empresa! 
Com o uso de Intelig√™ncia Artificial, voc√™ pode realizar consultas r√°pidas e obter respostas detalhadas baseadas nos documentos dispon√≠veis.
""")

# Adiciona uma barra lateral com instru√ß√µes
with st.sidebar:
    st.header("Instru√ß√µes")
    st.markdown("""
    1. Digite sua pergunta no campo ao lado.
    2. Aguarde enquanto o assistente processa sua consulta.
    3. Receba uma resposta detalhada baseada nos documentos da empresa.
    4. Sistemas de IA podem cometer erros.
    5. Sempre verifique as respostas.
    """)
    st.write("Exemplos de perguntas:")
    st.write("- Como √© feita a coleta de informa√ß√µes de ponto e frequ√™ncia?")
    st.write("- Quando o departamento financeiro efetua o pagamento ao fornecedor?")
    st.write("- Por que usar o m√©todo FIFO (First In, First Out) no processo de log√≠stica?")

    # Rodap√© da aplica√ß√£o
    st.markdown("""
    ---
    üß† Data Science Academy - Projeto 8
    """)

# Inicializa a sess√£o de mensagens se n√£o existir
if "messages" not in st.session_state.keys():  
    st.session_state.messages = [
        {"role": "assistant", "content": "Ol√°! Como posso ajudar? Digite sua pergunta abaixo."}
    ]

# Define o LLM (Large Language Model)
llm = Ollama(model = "llama3", request_timeout = 600.0)  

# Define o modelo de embeddings
embed_model = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2")

# Fun√ß√£o para o M√≥dulo de RAG com LlamaIndex, cacheada pelo Streamlit
@st.cache_resource(show_spinner = False)
def dsa_modulo_rag():

    # Exibe um spinner durante o carregamento
    with st.spinner(text = "Carregando e indexando os documentos Streamlit ‚Äì aguarde! Isso deve levar de 1 a 2 minutos."):  

        # L√™ documentos do diret√≥rio especificado
        reader = SimpleDirectoryReader(input_dir = "./documentos", recursive = True)  
        
        # Carrega os dados dos documentos
        docs = reader.load_data()  
        
        # Configura o LLM nos settings globais
        Settings.llm = llm  
        
        # Configura o modelo de embeddings nos settings globais
        Settings.embed_model = embed_model  
        
        # Cria um √≠ndice de vetores a partir dos documentos
        index = VectorStoreIndex.from_documents(docs)  
        
        # Retorna o √≠ndice criado
        return index  

# Carrega o √≠ndice de dados
index = dsa_modulo_rag() 

# Construindo o Motor de Execu√ß√£o da App com RAG
# Este c√≥digo verifica se um motor de chat j√° est√° inicializado na sess√£o do Streamlit. 
# Se n√£o estiver, ele o inicializa com as configura√ß√µes do banco vetorial e o armazena no session_state para uso cont√≠nuo durante a sess√£o do usu√°rio.
# chat_mode = "condense_question" gera um pergunta condensada sendo um modo de chat simples criado em cima de um mecanismo de consulta sobre seus dados.
# Essa abordagem funciona para perguntas diretamente relacionadas √† base de conhecimento. 
# Como ele sempre consulta a base de conhecimento, pode ter dificuldade em responder a metaperguntas como "o que eu perguntei a voc√™ antes?"
# https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_condense_question/
if "chat_engine" not in st.session_state.keys():  

    # Inicializa o motor de chat
    st.session_state.chat_engine = index.as_chat_engine(chat_mode = "condense_question", verbose = True)  

# Verifica se h√° uma entrada de chat do usu√°rio
if prompt := st.chat_input("Sua pergunta"):  

    # Adiciona a mensagem do usu√°rio ao estado da sess√£o
    st.session_state.messages.append({"role": "user", "content": prompt})  

# Exibe as mensagens na interface de chat
for message in st.session_state.messages: 

    # Cria um componente de mensagem na interface 
    with st.chat_message(message["role"]):  

        # Exibe o conte√∫do da mensagem
        st.write(message["content"])  

# Se a √∫ltima mensagem n√£o for do assistente, gera uma resposta
if st.session_state.messages[-1]["role"] != "assistant":

    # Cria um componente de mensagem para o assistente
    with st.chat_message("assistant"):

        # Exibe um spinner enquanto pensa
        with st.spinner("Pensando..."):
            
            # Engenharia de prompt para melhor contextualiza√ß√£o
            user_message = st.session_state.messages[-1]["content"]
            
            contextual_prompt = f"Voc√™ √© um assistente de busca especializado. O usu√°rio fez a seguinte pergunta: '{user_message}'. Considere todos os documentos dispon√≠veis e forne√ßa uma resposta detalhada e precisa."
            
            # Gera uma resposta usando o motor de chat
            response = st.session_state.chat_engine.chat(contextual_prompt)

            # Exibe a resposta gerada
            st.write(response.response)

            # Adiciona a mensagem do assistente ao estado da sess√£o
            st.session_state.messages.append({"role": "assistant", "content": response.response})


# Fim

# Exemplos de perguntas:

# Como √© feita a coleta de informa√ß√µes de ponto e frequ√™ncia?
# Como √© √© feito o c√°lculo de sal√°rios?
# Quando o departamento financeiro efetua o pagamento ao fornecedor?
# Por que usar o m√©todo FIFO (First In, First Out) no processo de log√≠stica?
# Como √© feito o monitoramento cont√≠nuo dos n√≠veis de mercadorias armazenadas?

